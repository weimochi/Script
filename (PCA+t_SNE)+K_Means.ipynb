{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import package"
      ],
      "metadata": {
        "id": "RJ4-yAU_TMyy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL2nqidUPriO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install bioinfokit\n",
        "from bioinfokit.visuz import cluster\n",
        "from sklearn.manifold import TSNE\n",
        "from keras.datasets import mnist\n",
        "from numpy import reshape\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import file and shapio test"
      ],
      "metadata": {
        "id": "Cc8rnk_6T_N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('') #encoding= 'unicode_escape')   #讀取資料 import your data here\n",
        "df1.isnull().values  #缺失值\n",
        "~df1.applymap(np.isreal).values  #非數字\n",
        "\n",
        "from scipy.stats import shapiro\n",
        "Y = df1.iloc[:, 1:].values\n",
        "shapiro(Y)"
      ],
      "metadata": {
        "id": "oeAzFw_ZVaMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "correlation"
      ],
      "metadata": {
        "id": "XELJOlUnWv1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df1.corr(method='spearman', min_periods=1)  #以spearman的方式找相關係數，所有對角值均為1.0\n",
        "print(correlation_matrix)\n",
        "#correlation matrix\n",
        "f, ax = plt.subplots(figsize=(16, 12))\n",
        "sns.heatmap(correlation_matrix, vmax=.9, cbar=True, annot=True,square=True)  #放大圖片"
      ],
      "metadata": {
        "id": "Fz4WsVHyW2IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA"
      ],
      "metadata": {
        "id": "DntU0gz3XEnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1.iloc[:, 1:].values #去除了去除了id那一行\n",
        "#print(X)\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X)\n",
        "\n",
        "#取最佳pca降維\n",
        "pcan = PCA(n_components='mle')\n",
        "X_train_pca = pcan.fit_transform(X_train_std)\n",
        "print(pcan.explained_variance_ratio_) \n",
        "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1]) \n",
        "#這邊看pc，可以改成 X_train_pca[:, 0], X_train_pca[:, 2]，這樣可以看這樣可以看PC1.3的結果\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "pca9 = PCA(0.9)\n",
        "X_train_pca = pca9.fit_transform(X_train_std) #到PC90%\n",
        "print(pca9.explained_variance_ratio_)\n",
        "print(pca9.components_)\n",
        "\n",
        "# 將PC結果變成table\n",
        "\n",
        "pca_out = PCA('mle').fit(X_train_std)\n",
        "loadings = pca_out.components_\n",
        "num_pc = pca_out.n_features_\n",
        "pc_list = [\"PC\"+str(i) for i in list(range(1, num_pc+1))]\n",
        "loadings_df = data_dum_1.iloc[:, 1:].from_dict(dict(zip(pc_list, loadings)))\n",
        "loadings_df['variable'] = data_dum_1.iloc[:, 1:].columns.values\n",
        "loadings_df = loadings_df.set_index('variable')\n",
        "loadings_df\n",
        "# output\n",
        "\n",
        "# 畫pca向量圖\n",
        "cluster.pcaplot(x=loadings[0], y=loadings[1], labels=data_dum_1.iloc[:, 1:].columns.values, \n",
        "    var1=round(pca_out.explained_variance_ratio_[0]*100, 2),\n",
        "    var2=round(pca_out.explained_variance_ratio_[1]*100, 2), r = 700)\n",
        "# get PC scores\n",
        "pca_scores = PCA().fit_transform(X_train_std)\n",
        "\n",
        "# get 2D biplot\n",
        "cluster.biplot(cscore=pca_scores, loadings=loadings, labels=data_dum_1.iloc[:, 1:].columns.values, \n",
        "                   var1=round(pca_out.explained_variance_ratio_[0]*100, 2),\n",
        "                   var2=round(pca_out.explained_variance_ratio_[1]*100, 2),arrowcolor='dimgray', colordot ='lightcoral' ,\n",
        "                              valphaarrow = 1, arrowlinewidth = 0.8, r = 700) #, colorlist = target)\n",
        "'''\n",
        "#合併兩張圖\n",
        "from skimage.io import imread\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "pcaplot = imread('/content/pcaplot_2d.png')\n",
        "biplot = imread('/content/biplot_2d.png')\n",
        "\n",
        "plt.subplot(121), plt.imshow(pcaplot), plt.axis('off'), plt.title('PCA plot', size=20)\n",
        "plt.subplot(122), plt.imshow(biplot), plt.axis('off'), plt.title('PCA biplot', size=20)\n",
        "plt.show() \n",
        "'''        "
      ],
      "metadata": {
        "id": "gigi6sMkYlPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pca+k-means"
      ],
      "metadata": {
        "id": "_ovLOr1CdlyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 產生的資料組數 (10)\n",
        "clusters = 10\n",
        "# K 值的範圍 (2~10)\n",
        "k_range = range(2, clusters + 1)\n",
        "dx, dy = X_train_pca[:, 0],  X_train_pca[:, 1]\n",
        "distortions = []\n",
        "scores = []\n",
        "\n",
        "# 記錄每種 K 值建出的 KMeans 模型的成效\n",
        "for i in k_range:\n",
        "    kmeans = KMeans(n_clusters=i,random_state=0).fit(X_train_pca)\n",
        "    distortions.append(kmeans.inertia_) # 誤差平方和 (SSE)\n",
        "    scores.append(silhouette_score(X_train_pca, kmeans.predict(X_train_pca))) # 側影係數\n",
        "# 找出最大的側影係數來決定 K 值\n",
        "selected_K = scores.index(max(scores)) + 2\n",
        "# 重新建立 KMeans 模型並預測目標值\n",
        "kmeans = KMeans(n_clusters=selected_K).fit(X_train_pca)\n",
        "new_dy = kmeans.predict(X_train_pca)\n",
        "# 新分組的資料中心點\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.figure(figsize=(12, 12))\n",
        "# 原始資料分組\n",
        "plt.subplot(221)\n",
        "plt.title(f'Original data')\n",
        "plt.scatter(X_train_pca.T[0], X_train_pca.T[1])\n",
        "#plt.title(f'Original data ({clusters} groups)')\n",
        "#plt.scatter(X_train_pca.T[0], X_train_pca.T[1], c=dy, cmap=plt.cm.Set1)\n",
        "# 新資料分組\n",
        "plt.subplot(222)\n",
        "plt.title(f'KMeans={selected_K} groups')\n",
        "plt.scatter(X_train_pca.T[0], X_train_pca.T[1], c=new_dy, cmap=plt.cm.Set3)\n",
        "plt.scatter(centers.T[0], centers.T[1], marker='^', color='orange')\n",
        "for i in range(centers.shape[0]): # 標上各分組中心點\n",
        "    plt.text(centers.T[0][i], centers.T[1][i], str(i + 1),\n",
        "             fontdict={'color': 'red', 'weight': 'bold', 'size': 24})\n",
        "# 繪製誤差平方和圖 (手肘法)\n",
        "plt.subplot(223)\n",
        "plt.title('SSE (elbow method)')\n",
        "plt.plot(k_range, distortions)\n",
        "plt.plot(selected_K, distortions[selected_K - 2], 'go') # 最佳解\n",
        "# 繪製係數圖\n",
        "plt.subplot(224)\n",
        "plt.title('Silhouette score')\n",
        "plt.plot(k_range, scores)\n",
        "plt.plot(selected_K, scores[selected_K - 2], 'go') # 最佳解\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p9bLATqYdq16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE"
      ],
      "metadata": {
        "id": "5ZNrk4PIbbS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a dataframe from the dataset\n",
        "df = pd.DataFrame(data = X_train_pca,\n",
        "                 columns = [\"Component 1\", \n",
        "                            \"Component 2\",\n",
        "                            \"Component 3\",\n",
        "                            \"Component 4\",\n",
        "                            \"Component 5\",\n",
        "                            \"Component 6\",\n",
        "                            \"Component 7\",\n",
        "                            \"Component 8\",\n",
        "                            \"Component 9\"\n",
        "                            ])  #這邊會隨著資料不同而有所增減\n",
        "\n",
        "#merge this with the data_dum_1 data\n",
        "data_dum_1 = pd.merge(data_dum_1,\n",
        "              df,\n",
        "              left_index=True,\n",
        "              right_index=True,\n",
        "              how = \"inner\")\n",
        "\n",
        "#set the hyperparmateres for the model with\n",
        "#the same as in the workshop\n",
        "#try playing around with them\n",
        "#how does that affect the outcome?\n",
        "#自行調整參數\n",
        "keep_dims = 2\n",
        "lrn_rate = 'auto' #5000\n",
        "#The ‘auto’ option sets the learning_rate to max(N / early_exaggeration / 4, 50)\n",
        "# where N is the sample size, following [4] and [5].\n",
        "prp = 40\n",
        "\n",
        "#create the model\n",
        "tsne = TSNE(n_components = keep_dims, \n",
        "            perplexity = prp, \n",
        "            random_state = 42,\n",
        "            n_iter = 5000,\n",
        "            n_jobs = -1)\n",
        "\n",
        "tsne_df = data_dum_1.loc[:,\"Component 1\":f\"Component 9\"].copy()\n",
        "\n",
        "#fit the model\n",
        "X_embedded = tsne.fit_transform(tsne_df)\n",
        "\n",
        "#create a dataframe from the dataset\n",
        "df = pd.DataFrame(data = X_embedded,\n",
        "                 columns = [\"Dimension 1\", \n",
        "                            \"Dimension 2\"])\n",
        "\n",
        "#merge this with the data_dum_1 data\n",
        "data_dum_1 = pd.merge(data_dum_1,\n",
        "              df,\n",
        "              left_index=True,\n",
        "              right_index=True,\n",
        "              how = \"inner\")\n",
        "\n",
        "data_dum_1.columns\n",
        "data_dum_1.head()\n",
        "\n",
        "#color and plot the result, for example\n",
        "'''\n",
        "g = sns.jointplot(data = data_dum_1,\n",
        "                 x = \"Dimension 1\",\n",
        "                 y = \"Dimension 2\", \n",
        "                 hue = \"Ch07\")\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "oN8cidDzeXIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tsne+kmeans"
      ],
      "metadata": {
        "id": "dDmnCV8Hf0ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 產生的資料組數 (10)\n",
        "clusters = 10\n",
        "# K 值的範圍 (2~10)\n",
        "k_range = range(2, clusters + 1)\n",
        "dx, dy = X_embedded[:,0], X_embedded[:,1]\n",
        "distortions = []\n",
        "scores = []\n",
        "\n",
        "# 記錄每種 K 值建出的 KMeans 模型的成效，random state = 10是6組，9是2組\n",
        "for i in k_range:\n",
        "    kmeans = KMeans(n_clusters=i,random_state=10).fit(X_embedded)\n",
        "    distortions.append(kmeans.inertia_) # 誤差平方和 (SSE)\n",
        "    scores.append(silhouette_score(X_embedded, kmeans.predict(X_embedded))) # 側影係數\n",
        "# 找出最大的側影係數來決定 K 值\n",
        "selected_K = scores.index(max(scores)) + 2\n",
        "# 重新建立 KMeans 模型並預測目標值\n",
        "kmeans = KMeans(n_clusters=selected_K).fit(X_embedded)\n",
        "new_dy = kmeans.predict(X_embedded)\n",
        "# 新分組的資料中心點\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.figure(figsize=(12, 12))\n",
        "# 原始資料分組\n",
        "plt.subplot(221)\n",
        "plt.title(f'Original data')\n",
        "plt.scatter(X_embedded[:,0], X_embedded[:,1]) #X_embedded.T[1], , c=dy, cmap=plt.cm.Set1\n",
        "#to see the 10 group, please copy under:\n",
        "# plt.title(f'Original data={clusters} groups')\n",
        "# plt.scatter(X_embedded.T[:,0], X_embedded.T[:,1], c=dy, cmap=plt.cm.Set1) \n",
        "# 新資料分組\n",
        "plt.subplot(222)\n",
        "plt.title(f'KMeans={selected_K} groups')\n",
        "plt.scatter(X_embedded.T[0], X_embedded.T[1], c=new_dy, cmap=plt.cm.Set3)\n",
        "plt.scatter(centers.T[0], centers.T[1], marker='^', color='orange')\n",
        "for i in range(centers.shape[0]): # 標上各分組中心點\n",
        "    plt.text(centers.T[0][i], centers.T[1][i], str(i + 1),\n",
        "             fontdict={'color': 'red', 'weight': 'bold', 'size': 24})\n",
        "# 繪製誤差平方和圖 (手肘法)\n",
        "plt.subplot(223)\n",
        "plt.title('SSE (elbow method)')\n",
        "plt.plot(k_range, distortions)\n",
        "plt.plot(selected_K, distortions[selected_K - 2], 'go') # 最佳解\n",
        "# 繪製係數圖\n",
        "plt.subplot(224)\n",
        "plt.title('Silhouette score')\n",
        "plt.plot(k_range, scores)\n",
        "plt.plot(selected_K, scores[selected_K - 2], 'go') # 最佳解\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2u312ptTf3qM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}